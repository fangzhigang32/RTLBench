# RTLBench: A Multi-Dimensional Benchmark Suite for Evaluating LLM-Generated RTL Code

![Evaluation Flow](./LintEval_Overview.png)

---

## üóÇÔ∏è Repository Overview

This repository consists of two main components:

- [`AutoEval`](./AutoEval)  
  Automated evaluation framework for analyzing RTL code.

- [`RTLBench-Benchmark`](./RTLBench-Benchmark)  
  A comprehensive benchmark suite for assessing code generation capabilities of large language models (LLMs) in RTL design.

---

## üìÅ AutoEval Folder Structure

The `AutoEval` directory contains two key submodules:

### 1. [`EvalExistBenchmark`](./AutoEval/EvalExistBenchmark)

Evaluate the quality of reference RTL code in existing benchmarks.

- **Run Command**:
  ```bash
  python verilatorLint.py
  ```

### 2. [`RTLBench`](./AutoEval/RTLBench)

Use RTLBench-Benchmark to evaluate LLM-generated RTL code.

- **Configuration**:  
  Edit [`api.txt`](./AutoEval/RTLBench/api.txt) to set your `API_KEY` and `BaseURL`.

- **Run Command**:
  ```bash
  python main.py
  ```

---

## üêç Python Environment

- **Required Version**: Python `3.13.2`

---

## üì¶ Dependency Installation

Install project dependencies using the provided `requirements.txt`:

```bash
pip install -r AutoEval/requirements.txt
```

---

## üì¨ Contributions

Feel free to open issues or submit pull requests to improve the benchmark suite and evaluation tools.
