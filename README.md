# RTLBench: A Multi-Dimensional Benchmark Suite for Evaluating LLM-Generated RTL Code

![Evaluation Flow](./LintEval_Overview.png)

---

## ğŸ—‚ï¸ Repository Overview

This repository consists of two main components:

- `AutoEval`
  Automated evaluation framework for analyzing RTL code.

- `RTLBench-Benchmark` 
  A comprehensive benchmark suite for assessing code generation capabilities of large language models (LLMs) in RTL design.

---

## ğŸ“ AutoEval Folder Structure

The `AutoEval` directory contains two key submodules:

### 1. `EvalExistBenchmark`

Evaluate the quality of reference RTL code in existing benchmarks.

- **Run Command**:
  ```bash
  python verilatorLint.py
  ```

### 2. `RTLBench`

Use RTLBench-Benchmark to evaluate LLM-generated RTL code.

- **Configuration**:  
  Edit [`api.txt`](./AutoEval/RTLBench/code/api.txt) to set your `API_KEY` and `BaseURL`.

- **Run Command**:
  ```bash
  python main.py
  ```
- **Experimental Results**:
  The experimental results will be stored in the `experiment/model_name/log` folder.
  - `summary_report.txt` file stores the results of the first evaluation.
  - `re_summary_report.txt` file stores the results of Log2BetterRTL post-optimization evaluation.
---

## ğŸ Python Environment

- **Required Version**: Python `3.13.2`

---

## ğŸ“¦ Dependency Installation

Install project dependencies using the provided [`requirements.txt`](./AutoEval/requirements.txt):

```bash
pip install -r AutoEval/requirements.txt
```

---

## ğŸ“¬ Contributions

Feel free to open issues or submit pull requests to improve the benchmark suite and evaluation tools.




----
<!-- è¿™é‡Œå¼€å§‹æ˜¯å¯æ»šåŠ¨è¡¨æ ¼ -->
<div style="max-height: 200px; overflow: auto; margin-bottom: 20px;">
  
| å§“å | å¹´é¾„ | èŒä¸š | åŸå¸‚ | è–ªèµ„ | å…¥èŒæ—¥æœŸ | é‚®ç®± |
|------|------|------|------|------|----------|------|
| å¼ ä¸‰ | 28 | è½¯ä»¶å·¥ç¨‹å¸ˆ | åŒ—äº¬ | ï¿¥15,000 | 2020-03-15 | zhangsan@example.com |
| æå›› | 35 | äº§å“ç»ç† | ä¸Šæµ· | ï¿¥20,000 | 2018-06-20 | lisi@example.com |
| ç‹äº” | 42 | é”€å”®æ€»ç›‘ | å¹¿å· | ï¿¥25,000 | 2015-11-03 | wangwu@example.com |
| èµµå…­ | 26 | UIè®¾è®¡å¸ˆ | æ·±åœ³ | ï¿¥12,000 | 2021-09-12 | zhaoliu@example.com |
| é’±ä¸ƒ | 31 | æ•°æ®åˆ†æå¸ˆ | æ­å· | ï¿¥18,000 | 2019-04-25 | qianqi@example.com |
| å­™å…« | 29 | å‰ç«¯å¼€å‘ | å—äº¬ | ï¿¥16,000 | 2020-08-17 | sunba@example.com |
| å‘¨ä¹ | 38 | åç«¯å¼€å‘ | æˆéƒ½ | ï¿¥19,000 | 2017-02-14 | zhoujiu@example.com |
| å´å | 33 | è¿ç»´å·¥ç¨‹å¸ˆ | æ­¦æ±‰ | ï¿¥17,000 | 2018-12-30 | wushi@example.com |
| éƒ‘åä¸€ | 27 | æµ‹è¯•å·¥ç¨‹å¸ˆ | è¥¿å®‰ | ï¿¥13,000 | 2021-01-22 | zhengshiyi@example.com |
| ç‹åäºŒ | 36 | æŠ€æœ¯ç»ç† | è‹å· | ï¿¥23,000 | 2016-07-08 | wangshier@example.com |
| åˆ˜åä¸‰ | 30 | å¸‚åœºè¥é”€ | å¤©æ´¥ | ï¿¥14,000 | 2019-11-11 | liushisan@example.com |
| é™ˆåå›› | 39 | äººåŠ›èµ„æº | é‡åº† | ï¿¥16,500 | 2015-05-19 | chenshisi@example.com |

</div>
